{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f90966b5",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f189cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_art import *\n",
    "import openai\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sklearn.manifold import TSNE\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0bbef9",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a451e5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LMM Querries\n",
    "\n",
    "def apply_completions(input_dict,display=False):\n",
    "    chat_completion = llm_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": input_dict[\"i_role\"],\n",
    "                \"content\": input_dict[\"i_content\"],\n",
    "                \"name\": input_dict[\"i_name\"],\n",
    "            }\n",
    "        ],\n",
    "        model=input_dict[\"i_model\"],\n",
    "        temperature=input_dict[\"i_temperature\"],\n",
    "        max_tokens=input_dict[\"i_token_max\"],\n",
    "        n=input_dict[\"i_n\"],\n",
    "        seed=input_dict[\"i_seed\"],\n",
    "        frequency_penalty=input_dict[\"i_frequency_penalty\"],\n",
    "        presence_penalty=input_dict[\"i_presence_penalty\"]\n",
    "    )\n",
    "    if display :\n",
    "        print(chat_completion)\n",
    "    return chat_completion\n",
    "\n",
    "def apply_embeddings(input_dict,display=False):\n",
    "    text_embeddings = llm_client.embeddings.create(\n",
    "        input=input_dict[\"i_text\"],\n",
    "        model=input_dict[\"i_model\"],\n",
    "        encoding_format=input_dict[\"i_encoding_format\"],\n",
    "        dimensions=input_dict[\"i_dimensions\"],\n",
    "        user=input_dict[\"i_user\"])\n",
    "    if display :\n",
    "        print(text_embeddings)\n",
    "    return text_embeddings\n",
    "\n",
    "## Ceate Input Conf\n",
    "\n",
    "def llmInputConfCompletion(content,role_num=0,model_num=0,temperature=1,max_tokens=2000,num_answer=1,seed=0, hash_key=None) :\n",
    "    return {\"i_content\":content,\n",
    "            \"i_role\":role_list[role_num],\n",
    "            \"i_model\":model_list[model_num],\n",
    "            \"i_temperature\":temperature,\n",
    "            \"i_token_max\":max_tokens,\n",
    "            \"i_n\":num_answer,\n",
    "            \"i_seed\":seed,\n",
    "            \"i_name\":\"name_test\",\n",
    "            \"i_frequency_penalty\":0,\n",
    "            \"i_presence_penalty\":0,\n",
    "            \"hash_key\":hash_key}\n",
    "\n",
    "def llmInputConfEmbeddings(content, model=\"text-embedding-3-small\", encoding_format=\"float\", dimensions=10, hash_key=None) :\n",
    "    return {\"i_text\":content,\n",
    "            \"i_model\":model,\n",
    "            \"i_encoding_format\":encoding_format,\n",
    "            \"i_dimensions\":dimensions,\n",
    "            \"i_user\":\"name_test\",\n",
    "            \"hash_key\":hash_key}\n",
    "\n",
    "## LLM Querry output Parsing\n",
    "\n",
    "def outputDictParseCompletion(output,display=False) :\n",
    "    out_dict = {}\n",
    "    gpt_dict = dict(output)\n",
    "    out_dict[\"o_id\"] = gpt_dict[\"id\"]\n",
    "    out_dict[\"o_system_fingerprint\"] = gpt_dict[\"system_fingerprint\"]\n",
    "    out_dict[\"o_logprobs\"] = dict(gpt_dict[\"choices\"][0])[\"logprobs\"]\n",
    "    out_dict[\"o_model\"] = gpt_dict[\"model\"]\n",
    "    out_dict[\"o_object\"] = gpt_dict[\"object\"]\n",
    "    out_dict[\"o_created\"] = gpt_dict[\"created\"]\n",
    "    out_dict[\"o_finish_reason\"] = dict(gpt_dict[\"choices\"][0])[\"finish_reason\"]\n",
    "    out_dict[\"o_index\"] = dict(gpt_dict[\"choices\"][0])[\"index\"]\n",
    "    out_dict[\"o_content\"] = dict(dict(gpt_dict[\"choices\"][0])[\"message\"])[\"content\"]\n",
    "    out_dict[\"o_role\"] = dict(dict(gpt_dict[\"choices\"][0])[\"message\"])[\"role\"]\n",
    "    out_dict[\"o_object\"] = gpt_dict[\"object\"]\n",
    "    out_dict[\"o_token_output\"] = dict(gpt_dict[\"usage\"])[\"completion_tokens\"]\n",
    "    out_dict[\"o_token_input\"] = dict(gpt_dict[\"usage\"])[\"prompt_tokens\"]\n",
    "    out_dict[\"o_token_total\"] = dict(gpt_dict[\"usage\"])[\"total_tokens\"]\n",
    "    if display :\n",
    "        print(out_dict)\n",
    "    return out_dict\n",
    "        \n",
    "def outputDictParseEmbeddings(output,display=False) :\n",
    "    out_dict = {}\n",
    "    gpt_dict = dict(output)\n",
    "    out_dict[\"o_data\"] = dict(gpt_dict[\"data\"][0])[\"embedding\"]\n",
    "    out_dict[\"o_index\"] = dict(gpt_dict[\"data\"][0])[\"index\"]\n",
    "    out_dict[\"o_object\"] = dict(gpt_dict[\"data\"][0])[\"object\"]\n",
    "    out_dict[\"o_model\"] = gpt_dict[\"model\"]\n",
    "    out_dict[\"o_object_list\"] = gpt_dict[\"object\"]\n",
    "    out_dict[\"o_object_list\"] = gpt_dict[\"object\"]\n",
    "    out_dict[\"o_token_input\"] = dict(gpt_dict[\"usage\"])[\"prompt_tokens\"]\n",
    "    out_dict[\"o_token_total\"] = dict(gpt_dict[\"usage\"])[\"total_tokens\"]\n",
    "    if display :\n",
    "        print(out_dict)\n",
    "    return out_dict\n",
    "\n",
    "def parseList(list_par) :\n",
    "    output_str = \"\"\n",
    "    if type(list_par) == type([]) :\n",
    "        for i in list_par :\n",
    "            output_str = output_str + str(i)\n",
    "    elif type(list_par) == type(\"\") :\n",
    "        output_str = list_par\n",
    "    return str(output_str)\n",
    "\n",
    "\n",
    "def textListToText(text_list) :\n",
    "    out_list = \"\"\n",
    "    for text in text_list :\n",
    "        out_list = out_list + text\n",
    "    return out_list\n",
    "\n",
    "def llmInputConfArticle(article_text,llm_prompt) :\n",
    "    context_prompt = \"\\nHere is the article :\\n\"\n",
    "    final_prompt = str(llm_prompt)+str(context_prompt)+article_text\n",
    "    return llmInputConf(final_prompt)\n",
    "\n",
    "    \n",
    "def num_tokens_from_string(text=\"\", encoding_name=\"cl100k_base\"):\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(text))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def saveNP(data,fmt='%f'): #path,\n",
    "    np.savetxt(\"C:/Users/User/OneDrive/Desktop/article/file_2/test_llm_output/test_save.txt\",data, fmt=fmt)\n",
    "\n",
    "def loadNP(): #path\n",
    "    return np.loadtxt('C:/Users/User/OneDrive/Desktop/article/file_2/test_llm_output/test_save.txt', dtype=float)\n",
    "\n",
    "def plot3Dpn(np_data):\n",
    "    fig = px.scatter_3d(x=np_data[:, 0], y=np_data[:, 1], z=np_data[:, 2],color=np_data[:, 3], opacity=0.8)\n",
    "    fig.show()\n",
    "\n",
    "def plotTSNE(data,n_components=2,perplexity=3,random_state=10):\n",
    "    tsne = TSNE(n_components=n_components,perplexity=perplexity,random_state=random_state) # , random_state=100\n",
    "    X_tsne = tsne.fit_transform(data)\n",
    "    print(tsne.kl_divergence_)\n",
    "    fig = px.scatter(x=X_tsne[:, 0], y=X_tsne[:, 1]) #, color=np.array(range(69))\n",
    "    fig.update_layout(\n",
    "        title=\"t-SNE visualization of Custom Classification dataset\",\n",
    "        xaxis_title=\"First t-SNE\",\n",
    "        yaxis_title=\"Second t-SNE\",\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "def getDataToQuerryListLLM(max_prompt=5,articleTrueQuestionsFalse=True) :\n",
    "    out_dict_List = []\n",
    "    if articleTrueQuestionsFalse :\n",
    "        getNumberOfArticles(open_path)\n",
    "        filename_list = loadArticleFolderList(open_path,max_prompt) # [\"fa897c02295f34ce2e15f602769edf204ea00be7.txt\"]\n",
    "        out_dict_List = loadListArticleHash(open_path,filename_list)\n",
    "    else :\n",
    "        prompt_list = cfn_field(\"prompts\",\"prompt_type\",\"content\",\"prompt_value\",max_prompt)\n",
    "        for prompt in prompt_list:\n",
    "            hash_key = hashlib.shake_256(str(prompt).encode()).hexdigest(20)\n",
    "            out_dict_List.append({\"hash_key\":hash_key,\"text\":prompt})\n",
    "    return out_dict_List\n",
    "\n",
    "def testQuestionsBatchCompletion():\n",
    "    articleTrueQuestionsFalse = True\n",
    "    completionTrueEmbeddingFalse = False\n",
    "    model_list = [0] # [0,1,2]\n",
    "    temperature_list = [0.5] # [0,0.25,0.5,0.75,1]\n",
    "    max_prompt = 45000# 51500 #  #100\n",
    "    save_every = 100\n",
    "    token_max_emb = 7500\n",
    "    cara_max_emb = 100\n",
    "    dim=10 # 100\n",
    "    df=None\n",
    "    set_index_key = \"hash_key\" #'o_created' #\"hash_key\"\n",
    "    prompt_list = getDataToQuerryListLLM(max_prompt,articleTrueQuestionsFalse)\n",
    "    prompt_list = prompt_list[0:100]\n",
    "    # prompt_list = cfn_field(\"prompts\",\"prompt_type\",\"content\",\"prompt_value\",max_prompt) #\n",
    "    count = 0\n",
    "    for prompt in prompt_list:\n",
    "        for model_n in model_list:\n",
    "            for temperature_n in temperature_list :\n",
    "                valid_dict = {\"valid\":\"VALID\"}\n",
    "                if completionTrueEmbeddingFalse :\n",
    "                    \n",
    "                    input_dict = llmInputConfCompletion(prompt[\"text\"],model_num=model_n,temperature=temperature_n,hash_key=prompt[\"hash_key\"])\n",
    "                    out_raw = apply_completions(input_dict)\n",
    "                    out_dict = outputDictParseCompletion(out_raw)\n",
    "                    selected_fields = selected_fields_comp\n",
    "                else :\n",
    "                    num_tokens = num_tokens_from_string(prompt[\"text\"])\n",
    "                    if num_tokens > token_max_emb :\n",
    "                        valid_dict = {\"valid\":\"WARNING\"}\n",
    "                        prompt[\"text\"] = prompt[\"text\"][0:cara_max_emb]\n",
    "                    print(\" - #\"+str(count),\"- \",valid_dict,\"-\",num_tokens,\"-\",len(prompt[\"text\"]),\"-\",prompt[\"hash_key\"])\n",
    "                    input_dict = llmInputConfEmbeddings(prompt[\"text\"],dimensions=dim,hash_key=prompt[\"hash_key\"])\n",
    "                    out_raw = apply_embeddings(input_dict)\n",
    "                    out_dict = outputDictParseEmbeddings(out_raw)\n",
    "                    selected_fields = selected_fields_emp\n",
    "                final_dict = input_dict | out_dict | valid_dict\n",
    "                df = addDictToDF(df,final_dict,selected_fields)\n",
    "                if count%save_every == 0  and count != 0:\n",
    "                    saveDFcsv(df.set_index(set_index_key), save_path, filename_save+str(count),True)\n",
    "                count = count + 1\n",
    "    saveDFcsv(df.set_index(set_index_key), save_path, filename_save+\"final\",True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456b0f71",
   "metadata": {},
   "source": [
    "## Article Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07ab96de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadArticleFolderList(folder_path=\"\",cutoff=99999999) :\n",
    "    root_path = Path(folder_path)\n",
    "    file_list = os.listdir(root_path)\n",
    "    file_list = file_list[:cutoff]\n",
    "    return file_list\n",
    "\n",
    "def getNumberOfArticles(folder_path=\"\", display=True) :\n",
    "    root_path = Path(folder_path)\n",
    "    file_list = os.listdir(root_path)\n",
    "    file_list_len = len(file_list)\n",
    "    if display : \n",
    "        print(\"In folder : \",folder_path,\" found \",file_list_len,\" article files.\")\n",
    "    return \n",
    "\n",
    "def loadListArticleHash(folder_path=\"\",list_hash=[]) :\n",
    "    text_dict_list = []\n",
    "    for i in list_hash:\n",
    "        hash_name = i.replace(\".txt\",\"\")\n",
    "        text = openSTRtxt(folder_path+\"/\",hash_name)\n",
    "        text_loaded_list_len = len(text)\n",
    "        dict_entry = {\"hash_key\":hash_name,\"text\":str(textListToText(text))} #textListToText(\n",
    "        text_dict_list.append(dict_entry)\n",
    "    return text_dict_list\n",
    "\n",
    "def getStatsOnArticleText(article_text_list) :\n",
    "    out_dict = {}\n",
    "    out_dict[\"line_num\"] = len(article_text_list)\n",
    "    article_text = textListToText(article_text_list)\n",
    "    out_dict[\"char_num\"] =len(article_text)\n",
    "    char_list = [\"\\n\", \".\",\"?\",\"!\",'\"',\",\",\"“\",\"”\",\":\",\"–\",\"-\",\";\",\"http://\",\"https://\",\"$\",\"€\",\"|\"]\n",
    "    for char in char_list :\n",
    "        out_dict[char] = article_text.count(char)\n",
    "    return out_dict\n",
    "\n",
    "def getDataToQuerryListLLM(max_prompt=5,articleTrueQuestionsFalse=True) :\n",
    "    out_dict_List = []\n",
    "    if articleTrueQuestionsFalse :\n",
    "        getNumberOfArticles(open_path)\n",
    "        filename_list = loadArticleFolderList(open_path,max_prompt) # [\"fa897c02295f34ce2e15f602769edf204ea00be7.txt\"]\n",
    "        out_dict_List = loadListArticleHash(open_path,filename_list)\n",
    "    else :\n",
    "        prompt_list = cfn_field(\"prompts\",\"prompt_type\",\"content\",\"prompt_value\",max_prompt)\n",
    "        for prompt in prompt_list:\n",
    "            hash_key = hashlib.shake_256(str(prompt).encode()).hexdigest(20)\n",
    "            out_dict_List.append({\"hash_key\":hash_key,\"text\":prompt})\n",
    "    return out_dict_List\n",
    "\n",
    "\n",
    "def dictSelectKeyList(input_dict,selected_fields) :\n",
    "    out_dict = {}\n",
    "    for key, value in input_dict.items():\n",
    "        if key in selected_fields :\n",
    "            out_dict[key] = value\n",
    "    return out_dict\n",
    "\n",
    "def getStandardDfnumColumn(num=10):\n",
    "    return pd.DataFrame([], columns=[\"\"+str(x) for x in range(num)])\n",
    "\n",
    "def addDictToDF(df=None, ar_dict={},selected_fields=[]):\n",
    "    if selected_fields == [] :\n",
    "        selected_fields = ar_dict.keys()\n",
    "    else :\n",
    "        ar_dict = dictSelectKeyList(ar_dict,selected_fields)\n",
    "    if type(df) == type(None) :\n",
    "        df = pd.DataFrame([], columns = selected_fields) \n",
    "    df_add = pd.DataFrame([ar_dict], columns = selected_fields)\n",
    "    if True : # list(df_add.columns) == (df.columns) :\n",
    "        df = pd.concat([df,df_add]).reset_index(drop=True)\n",
    "    else :\n",
    "        print(\"WARNING : df could not be added because the columns list is different\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe43e99a",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af813161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat GPT\n",
    "openai.api_key = \"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\"\n",
    "# model_list = [\"gpt-3.5-turbo-16k\",\"gpt-4\",\"gpt-3.5-turbo-16k\",\"gpt-3.5-turbo-0125\",\"gpt-4-0125-preview\",\"gpt-3.5-turbo\",\"gpt-4-turbo-preview\",\"text-embedding-3-small\",\"gpt-4\",\"gpt-3.5-turbo-16k-1106\"]\n",
    "model_list = [\"gpt-3.5-turbo-0125\", \"gpt-3.5-turbo-16k\",\"gpt-4-0125-preview\"]\n",
    "role_list = [\"user\",\"system\", \"assistant\", \"tool\"]\n",
    "max_token=100\n",
    "\n",
    "llm_client = OpenAI(api_key=openai.api_key)\n",
    "\n",
    "# Paths\n",
    "open_path = \"C:/Users/User/OneDrive/Desktop/article/files_3/1_3_article_main/arc/\"\n",
    "# path_model_list = \"C:/Users/User/OneDrive/Desktop/article/file_2/.code_control/\"\n",
    "# filename_model_lis = \"code_conf_excel\"\n",
    "\n",
    "save_path = \"C:/Users/User/OneDrive/Desktop/article/files_3/2_1_embdedding_main/embd_df/\"\n",
    "filename_save = \"embd_out_main_test\"\n",
    "# select_fields = ['content', 'role', 'model', 'temperature', 'max_tokens', 'n', 'seed', 'id', 'model_o', 'object', 'finish_reason', 'index', 'content_o', 'role_o', 'token_c', 'token_p', 'token_t']\n",
    "select_fields_comp = ['o_id','i_content', 'o_content', 'i_role', 'i_model', 'i_temperature', 'i_token_max', 'i_n', 'i_seed', 'i_name', 'i_frequency_penalty', 'i_presence_penalty','o_system_fingerprint', 'o_logprobs', 'o_model', 'o_object', 'o_created', 'o_finish_reason', 'o_index', 'o_role', 'o_token_output', 'o_token_input', 'o_token_total', \"valid\"]\n",
    "select_fields_emb = ['o_index','i_text', 'i_model', 'o_object', 'o_object_list', 'i_encoding_format', 'i_dimensions', 'o_data', 'o_token_input', 'o_token_total', \"valid\"]\n",
    "# select_fields_emb = ['hash_key', 'i_model', 'i_dimensions','i_encoding_format','o_object', 'o_object_list','o_token_input', 'o_token_total','o_data']\n",
    "\n",
    "\n",
    "#selected_fields_comp = [\"hash_key\",'i_content', 'i_role', 'i_model', 'i_temperature', 'i_token_max', 'i_n', 'i_seed', 'i_name', 'i_frequency_penalty', 'i_presence_penalty', 'o_id', 'o_system_fingerprint', 'o_logprobs', 'o_model', 'o_object', 'o_created', 'o_finish_reason', 'o_index', 'o_content', 'o_role', 'o_token_output', 'o_token_input', 'o_token_total']\n",
    "selected_fields_comp = [\"hash_key\",'o_id', 'i_role', 'i_model', 'i_temperature', 'i_token_max', 'i_frequency_penalty', 'i_presence_penalty', 'o_created','i_content','o_content', 'o_token_output', 'o_token_input', 'o_token_total',\"valid\"]\n",
    "#selected_fields_emp = [\"hash_key\",'i_text', 'i_model', 'i_encoding_format', 'i_dimensions', 'i_user', 'o_data', 'o_index', 'o_object', 'o_model', 'o_object_list', 'o_token_input', 'o_token_total']\n",
    "selected_fields_emp = [\"hash_key\",'i_model', 'i_dimensions', 'i_encoding_format', 'i_user', 'o_data', 'o_token_input', 'o_token_total','valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bf79b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In folder :  C:/Users/User/OneDrive/Desktop/article/files_3/1_3_article_main/arc/  found  51541  article files.\n",
      " - #0 -  {'valid': 'VALID'} - 128 - 674 - 00014490e1fffb2f61162c06173c7f0f3d7dee9b\n",
      " - #1 -  {'valid': 'VALID'} - 1825 - 9220 - 0001da11c3d4f43777e677dea08376cfa870bba3\n",
      " - #2 -  {'valid': 'VALID'} - 687 - 3809 - 0002fddefffb1be3ae4ccf417609c7fd914fc71e\n",
      " - #3 -  {'valid': 'VALID'} - 1005 - 5256 - 00056f22b89032bf6cc8a40218c3e50577b66db2\n",
      " - #4 -  {'valid': 'VALID'} - 1583 - 7017 - 0005f299c50f22366b33c8229c9f157eef697ac6\n",
      " - #5 -  {'valid': 'VALID'} - 1139 - 5598 - 00077c89f7890b0aa4c70be1a72ca4eaef66a290\n",
      " - #6 -  {'valid': 'VALID'} - 957 - 4932 - 0009df22a92a15378b62cea86bc5d2bd6517f5e8\n",
      " - #7 -  {'valid': 'VALID'} - 1707 - 8672 - 000a9128934b4d82d8494436108ac580f90675c7\n",
      " - #8 -  {'valid': 'VALID'} - 664 - 3257 - 000c131312b986986d558a80cb755a1adfc8994f\n",
      " - #9 -  {'valid': 'VALID'} - 2421 - 11295 - 000e3729113d67a015467abfc4ff12ef415cf00f\n",
      " - #10 -  {'valid': 'VALID'} - 961 - 4834 - 000fe5b991dd8194f350a0879a704d679387b3cc\n",
      " - #11 -  {'valid': 'VALID'} - 525 - 2430 - 0011101f0b7c779d9a524a1888ed596d7a67d5ab\n",
      " - #12 -  {'valid': 'VALID'} - 796 - 4033 - 0012c2f5125ac7ced594250abf1f2d9e3a61ed43\n",
      " - #13 -  {'valid': 'VALID'} - 995 - 4759 - 0012de43ee595b5e7538edffe76c58a98650abaf\n",
      " - #14 -  {'valid': 'VALID'} - 193 - 1018 - 0014434282937872b3769d30695a54ff3ffbeb34\n",
      " - #15 -  {'valid': 'VALID'} - 1742 - 8172 - 00173183ff6b5d283e717d6f653fd747fea3a622\n",
      " - #16 -  {'valid': 'VALID'} - 785 - 3958 - 001872d12ff4b8d1770995d060e0f94d4a06d614\n",
      " - #17 -  {'valid': 'VALID'} - 1276 - 6502 - 001913ce24a3eff96f7f2525b0fc5355c70211cb\n",
      " - #18 -  {'valid': 'VALID'} - 2180 - 9328 - 001c0ad6f7d1b00667cb93856838292b79c7d17d\n",
      " - #19 -  {'valid': 'VALID'} - 610 - 2693 - 001c6285a8bd8b287b89ccb01030ca45b51d6b76\n",
      " - #20 -  {'valid': 'VALID'} - 461 - 2141 - 001d708fa5dadc9f1eb7bcdd8f3740449016de32\n",
      " - #21 -  {'valid': 'VALID'} - 1437 - 6202 - 001ece368288225f4d0f1a2c40db0cc9f560a5d8\n",
      " - #22 -  {'valid': 'VALID'} - 195 - 867 - 001f0d1da707ddb512ce4b0239ae505e083ecd72\n",
      " - #23 -  {'valid': 'VALID'} - 156 - 769 - 001fba1ec368a7d910d39e1903a41784feb67328\n",
      " - #24 -  {'valid': 'VALID'} - 1333 - 7097 - 0020ec3ce14be246ca2b51f2050f58dd47dd55c8\n",
      " - #25 -  {'valid': 'VALID'} - 1153 - 6383 - 0022c28f02c7c09a0f905f35f468d4bbcab8e86b\n",
      " - #26 -  {'valid': 'VALID'} - 3940 - 15079 - 0024b4a17c691b09f65860fd01def548e01e1e0a\n",
      " - #27 -  {'valid': 'VALID'} - 1255 - 6142 - 00261a4606eb491489c1c33dce7ed47880ba141b\n",
      " - #28 -  {'valid': 'VALID'} - 653 - 3235 - 0026ad415f4d5abc06d6dff453fcc434ec541248\n",
      " - #29 -  {'valid': 'VALID'} - 407 - 1762 - 0027dfa2134561b368b2252d70b1c89fe99d56a3\n",
      " - #30 -  {'valid': 'WARNING'} - 8172 - 100 - 00281756414ca5c9c26af4394df330abf4ec5105\n",
      " - #31 -  {'valid': 'VALID'} - 1458 - 7399 - 002824ea4c23067fe1e4bccbc7d26a5ed6a32096\n",
      " - #32 -  {'valid': 'VALID'} - 961 - 5336 - 002891ea1bd22bf39b4487c885671f3de1b27dc1\n",
      " - #33 -  {'valid': 'VALID'} - 1079 - 4827 - 0028d3b73d1e1a3f832c3e9185be0e622210cc8c\n",
      " - #34 -  {'valid': 'VALID'} - 1317 - 6895 - 00298b6964c2e21c418fa6d7fed2dab7a49f57df\n",
      " - #35 -  {'valid': 'VALID'} - 400 - 2050 - 002cbd1ff51a21e032f902593256c777d05bdcc6\n",
      " - #36 -  {'valid': 'VALID'} - 370 - 2034 - 002cf7364e65ff87e22939e67d752bf962e0406d\n",
      " - #37 -  {'valid': 'VALID'} - 554 - 2770 - 0033c8b698b296be076e104a96878b85dc1c6e98\n",
      " - #38 -  {'valid': 'VALID'} - 1035 - 4550 - 0037ac810a18d8bf68362ca660e36adc0328d6c6\n",
      " - #39 -  {'valid': 'VALID'} - 64 - 333 - 0039677a98102268452da4931ca97b517532b89a\n",
      " - #40 -  {'valid': 'VALID'} - 893 - 4592 - 003ae554b2f5914f7526cdf0cfd7f4e2b754d9be\n",
      " - #41 -  {'valid': 'VALID'} - 472 - 2240 - 003ae9ec385cbbb96eee37ca29bc06e3caba033d\n",
      " - #42 -  {'valid': 'VALID'} - 810 - 4300 - 003bd9f6106ee7d3446752dd1ae32ad7e3f8eaf3\n",
      " - #43 -  {'valid': 'VALID'} - 407 - 1756 - 00404bc759416f93970a3996d90c62f0609f6a89\n",
      " - #44 -  {'valid': 'VALID'} - 1275 - 6695 - 0041abebfdd507a245967df996b1b6fd7dfc6c5c\n",
      " - #45 -  {'valid': 'VALID'} - 878 - 4880 - 0044ab78ced527488b4b96416244e60ea113df95\n",
      " - #46 -  {'valid': 'VALID'} - 109 - 361 - 0045a63bc9b909428fd37efb99584b2584e5a197\n",
      " - #47 -  {'valid': 'VALID'} - 187 - 1004 - 00466cd8afe7b50104ef1d265ea7cd19f56756cf\n",
      " - #48 -  {'valid': 'VALID'} - 2991 - 12730 - 00471ff609c5cf92c85e88beaf7aabf139007b7b\n",
      " - #49 -  {'valid': 'VALID'} - 2192 - 10781 - 00490ee8ec7486291db3de2e28410354722d4e65\n",
      " - #50 -  {'valid': 'VALID'} - 1030 - 4723 - 004b8414f171fc3fca9eeeeb7d2105860c5c01af\n",
      " - #51 -  {'valid': 'VALID'} - 619 - 3367 - 004b969f115e4111c6d95837cb5925513214c9ea\n",
      " - #52 -  {'valid': 'VALID'} - 1265 - 6485 - 004d052c807016cd6d49b55090df4d80850d222a\n",
      " - #53 -  {'valid': 'VALID'} - 1805 - 8261 - 004fa4798c112ce22098d7dd65ec37a46e9a6bde\n",
      " - #54 -  {'valid': 'VALID'} - 1479 - 6606 - 0050e09ef8e68a632d0225507cbef2ca7239dca2\n",
      " - #55 -  {'valid': 'VALID'} - 1079 - 5820 - 00531d066cd0db47fc81dc50acefa67546fad44f\n",
      " - #56 -  {'valid': 'VALID'} - 1318 - 6573 - 00554bc45b3270103bb5cbda38a5914c0883024e\n",
      " - #57 -  {'valid': 'VALID'} - 876 - 4440 - 00573db2f17f6e868af2aca41f6d5784c47fbe89\n",
      " - #58 -  {'valid': 'VALID'} - 580 - 2880 - 00583d23615d56a0e3be71b079559b66017278e1\n",
      " - #59 -  {'valid': 'VALID'} - 892 - 4713 - 005849251d850fa5fced48aab75f16f9a8992330\n",
      " - #60 -  {'valid': 'VALID'} - 1706 - 8675 - 005a561808850e700bb1b9dc1ea725fbe6e4e5c2\n",
      " - #61 -  {'valid': 'VALID'} - 730 - 3579 - 005b575d29ac91d75672c799bc3c39d40e170b4d\n",
      " - #62 -  {'valid': 'VALID'} - 541 - 2959 - 005c55d85ac09cfb2b3b14e67f0f6ea5708eb44f\n",
      " - #63 -  {'valid': 'VALID'} - 1890 - 9281 - 005d79f16369612e8027670515adebc8bb8e6c33\n",
      " - #64 -  {'valid': 'VALID'} - 4297 - 18849 - 005dac395be30a707069a9fd5ed906511bf4b88a\n",
      " - #65 -  {'valid': 'VALID'} - 407 - 1939 - 0060e315c29474ec7ceb335cfea4873ad4a2b86f\n",
      " - #66 -  {'valid': 'VALID'} - 82 - 375 - 00614f707d58e1030a815b793f566472f2cdfb43\n",
      " - #67 -  {'valid': 'VALID'} - 2458 - 11707 - 0062b4cee36cb90f0ca34819ab53d00e3ad67bba\n",
      " - #68 -  {'valid': 'VALID'} - 702 - 3474 - 00636fba6644a05d7367280ff46eda0d38401ab5\n",
      " - #69 -  {'valid': 'VALID'} - 570 - 2960 - 0064c006769f474db72e303af03312bc4df2e5b1\n",
      " - #70 -  {'valid': 'VALID'} - 1027 - 5271 - 006644330c84c8d353107bfb19fcd1b62fa6dbc5\n",
      " - #71 -  {'valid': 'VALID'} - 4260 - 19889 - 00673dca16906e2c3258aa7ea946c1420c8f5be6\n",
      " - #72 -  {'valid': 'VALID'} - 2010 - 9231 - 006794db10d0a7afa9936b12a6de127f7691df45\n",
      " - #73 -  {'valid': 'VALID'} - 1639 - 8442 - 00681071680216fcd5e96716d125b00ee6d3fc79\n",
      " - #74 -  {'valid': 'VALID'} - 344 - 1563 - 006b1b5e405eb3f7023826b92562008e9633d392\n",
      " - #75 -  {'valid': 'VALID'} - 2966 - 14760 - 006b65836c84e29e13cb74da8a8ace1f0a2f3d4b\n",
      " - #76 -  {'valid': 'VALID'} - 1052 - 4657 - 006cdb2213acd12b28b8cfd4778f54961fa8cf34\n",
      " - #77 -  {'valid': 'VALID'} - 881 - 4549 - 006d0985184549f05a4c5dc99fc89128dcf1f47a\n",
      " - #78 -  {'valid': 'VALID'} - 6170 - 29414 - 006f0f1a1ba1b8c31be29ec7be3eae081c12b057\n",
      " - #79 -  {'valid': 'VALID'} - 225 - 913 - 006f61f435fa0750cf5a8e33250c2954aa2437ab\n",
      " - #80 -  {'valid': 'VALID'} - 944 - 4912 - 006fb87ee64e71dc80700ce05b9b0ca5e30ab2ff\n",
      " - #81 -  {'valid': 'VALID'} - 938 - 4437 - 006fc73cd72de4cc8fe893f1271d1a4b6bed01c5\n",
      " - #82 -  {'valid': 'VALID'} - 1324 - 5525 - 0071484b17c53c08b4e44719fc4aca7fbc957dd9\n",
      " - #83 -  {'valid': 'VALID'} - 320 - 1814 - 0071b9447871467e7f1a7fe12280743aeb9fbb55\n",
      " - #84 -  {'valid': 'VALID'} - 775 - 4215 - 0071c1fb53c19047c197e2a79434884669fc6941\n",
      " - #85 -  {'valid': 'VALID'} - 795 - 4023 - 0073c68f1f1f1c272d4f0162f5424e3fb2c26a39\n",
      " - #86 -  {'valid': 'VALID'} - 410 - 1999 - 00741245f9d617a351eaa3d35634988093669f02\n",
      " - #87 -  {'valid': 'VALID'} - 555 - 2793 - 0075f4b71b61f52d724f642e1d1adc4a8eb6ebcd\n",
      " - #88 -  {'valid': 'VALID'} - 1235 - 6155 - 0076d914ee60bd61681339cb9d7d506c32612fb7\n",
      " - #89 -  {'valid': 'VALID'} - 2335 - 11434 - 0078ffa6270c19b7d6561468e6168d46314d2006\n",
      " - #90 -  {'valid': 'VALID'} - 1015 - 3570 - 007a5e4609b436e4dcb6a510fa4b5bae41989a8e\n",
      " - #91 -  {'valid': 'VALID'} - 854 - 4663 - 007aa275cebc2dbccb4ecfcca06d9a3b011f992d\n",
      " - #92 -  {'valid': 'VALID'} - 214 - 1186 - 007be9af27478f2f951e13ec25345c3131992533\n",
      " - #93 -  {'valid': 'VALID'} - 921 - 4715 - 007e25a05dca9fea7825829b2e183e599b06391e\n",
      " - #94 -  {'valid': 'VALID'} - 266 - 1346 - 007e9726469387a3bff846e1423b5916e7c729e9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - #95 -  {'valid': 'VALID'} - 2626 - 11537 - 007f783ec1eeb3ce7e9f272147b9176ac74976dc\n",
      " - #96 -  {'valid': 'VALID'} - 13 - 58 - 007ffaefefb596983c5a5e3c7413c4f08f467147\n",
      " - #97 -  {'valid': 'VALID'} - 1101 - 6130 - 0080197b2c5c133bc7d151033c62b63bc6e24431\n",
      " - #98 -  {'valid': 'VALID'} - 63 - 332 - 008077044574d372f45fa3cf40925c2f076c538d\n",
      " - #99 -  {'valid': 'VALID'} - 244 - 1016 - 0082dfae04d1810ad2ab1b255d60b2f3fccafda6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash_key</th>\n",
       "      <th>i_model</th>\n",
       "      <th>i_dimensions</th>\n",
       "      <th>i_encoding_format</th>\n",
       "      <th>i_user</th>\n",
       "      <th>o_data</th>\n",
       "      <th>o_token_input</th>\n",
       "      <th>o_token_total</th>\n",
       "      <th>valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00014490e1fffb2f61162c06173c7f0f3d7dee9b</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>10</td>\n",
       "      <td>float</td>\n",
       "      <td>name_test</td>\n",
       "      <td>[0.21210048, -0.18016657, 0.37964204, 0.534707...</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>VALID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001da11c3d4f43777e677dea08376cfa870bba3</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>10</td>\n",
       "      <td>float</td>\n",
       "      <td>name_test</td>\n",
       "      <td>[0.40177205, -0.14725468, 0.25325447, 0.374156...</td>\n",
       "      <td>1825</td>\n",
       "      <td>1825</td>\n",
       "      <td>VALID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002fddefffb1be3ae4ccf417609c7fd914fc71e</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>10</td>\n",
       "      <td>float</td>\n",
       "      <td>name_test</td>\n",
       "      <td>[-0.42271063, -0.19955428, 0.46736315, 0.27025...</td>\n",
       "      <td>687</td>\n",
       "      <td>687</td>\n",
       "      <td>VALID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00056f22b89032bf6cc8a40218c3e50577b66db2</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>10</td>\n",
       "      <td>float</td>\n",
       "      <td>name_test</td>\n",
       "      <td>[-0.031058876, -0.59449315, 0.15048245, 0.4776...</td>\n",
       "      <td>1005</td>\n",
       "      <td>1005</td>\n",
       "      <td>VALID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0005f299c50f22366b33c8229c9f157eef697ac6</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>10</td>\n",
       "      <td>float</td>\n",
       "      <td>name_test</td>\n",
       "      <td>[0.13927634, 0.48015112, -0.44763523, 0.263514...</td>\n",
       "      <td>1583</td>\n",
       "      <td>1583</td>\n",
       "      <td>VALID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>007f783ec1eeb3ce7e9f272147b9176ac74976dc</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>10</td>\n",
       "      <td>float</td>\n",
       "      <td>name_test</td>\n",
       "      <td>[0.25194308, 0.5778578, -0.03144711, 0.5126748...</td>\n",
       "      <td>2626</td>\n",
       "      <td>2626</td>\n",
       "      <td>VALID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>007ffaefefb596983c5a5e3c7413c4f08f467147</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>10</td>\n",
       "      <td>float</td>\n",
       "      <td>name_test</td>\n",
       "      <td>[0.034725662, 0.28461066, 0.5089451, 0.6630383...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>VALID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0080197b2c5c133bc7d151033c62b63bc6e24431</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>10</td>\n",
       "      <td>float</td>\n",
       "      <td>name_test</td>\n",
       "      <td>[-0.08492449, 0.05426993, -0.24591771, 0.26135...</td>\n",
       "      <td>1101</td>\n",
       "      <td>1101</td>\n",
       "      <td>VALID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>008077044574d372f45fa3cf40925c2f076c538d</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>10</td>\n",
       "      <td>float</td>\n",
       "      <td>name_test</td>\n",
       "      <td>[0.31235015, -0.16369727, -0.018954735, -0.035...</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>VALID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0082dfae04d1810ad2ab1b255d60b2f3fccafda6</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>10</td>\n",
       "      <td>float</td>\n",
       "      <td>name_test</td>\n",
       "      <td>[0.2531715, 0.05950563, -0.30274996, 0.5205738...</td>\n",
       "      <td>244</td>\n",
       "      <td>244</td>\n",
       "      <td>VALID</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    hash_key                 i_model  \\\n",
       "0   00014490e1fffb2f61162c06173c7f0f3d7dee9b  text-embedding-3-small   \n",
       "1   0001da11c3d4f43777e677dea08376cfa870bba3  text-embedding-3-small   \n",
       "2   0002fddefffb1be3ae4ccf417609c7fd914fc71e  text-embedding-3-small   \n",
       "3   00056f22b89032bf6cc8a40218c3e50577b66db2  text-embedding-3-small   \n",
       "4   0005f299c50f22366b33c8229c9f157eef697ac6  text-embedding-3-small   \n",
       "..                                       ...                     ...   \n",
       "95  007f783ec1eeb3ce7e9f272147b9176ac74976dc  text-embedding-3-small   \n",
       "96  007ffaefefb596983c5a5e3c7413c4f08f467147  text-embedding-3-small   \n",
       "97  0080197b2c5c133bc7d151033c62b63bc6e24431  text-embedding-3-small   \n",
       "98  008077044574d372f45fa3cf40925c2f076c538d  text-embedding-3-small   \n",
       "99  0082dfae04d1810ad2ab1b255d60b2f3fccafda6  text-embedding-3-small   \n",
       "\n",
       "   i_dimensions i_encoding_format     i_user  \\\n",
       "0            10             float  name_test   \n",
       "1            10             float  name_test   \n",
       "2            10             float  name_test   \n",
       "3            10             float  name_test   \n",
       "4            10             float  name_test   \n",
       "..          ...               ...        ...   \n",
       "95           10             float  name_test   \n",
       "96           10             float  name_test   \n",
       "97           10             float  name_test   \n",
       "98           10             float  name_test   \n",
       "99           10             float  name_test   \n",
       "\n",
       "                                               o_data o_token_input  \\\n",
       "0   [0.21210048, -0.18016657, 0.37964204, 0.534707...           128   \n",
       "1   [0.40177205, -0.14725468, 0.25325447, 0.374156...          1825   \n",
       "2   [-0.42271063, -0.19955428, 0.46736315, 0.27025...           687   \n",
       "3   [-0.031058876, -0.59449315, 0.15048245, 0.4776...          1005   \n",
       "4   [0.13927634, 0.48015112, -0.44763523, 0.263514...          1583   \n",
       "..                                                ...           ...   \n",
       "95  [0.25194308, 0.5778578, -0.03144711, 0.5126748...          2626   \n",
       "96  [0.034725662, 0.28461066, 0.5089451, 0.6630383...            13   \n",
       "97  [-0.08492449, 0.05426993, -0.24591771, 0.26135...          1101   \n",
       "98  [0.31235015, -0.16369727, -0.018954735, -0.035...            63   \n",
       "99  [0.2531715, 0.05950563, -0.30274996, 0.5205738...           244   \n",
       "\n",
       "   o_token_total  valid  \n",
       "0            128  VALID  \n",
       "1           1825  VALID  \n",
       "2            687  VALID  \n",
       "3           1005  VALID  \n",
       "4           1583  VALID  \n",
       "..           ...    ...  \n",
       "95          2626  VALID  \n",
       "96            13  VALID  \n",
       "97          1101  VALID  \n",
       "98            63  VALID  \n",
       "99           244  VALID  \n",
       "\n",
       "[100 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = testQuestionsBatchCompletion()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9052de3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407a62d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9cd539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414947d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5586544f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76120183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd90df1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2982bc63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b123b551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e63b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825e3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bb16a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c6737d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f58a963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa08538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b65668",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_data,\"text\",\"h_name\",\"h_data\",\"c_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9a2783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "di = {\"x\":0,\"y\":1,'c':\"category\",'size':'text_len','symbol':\"&\",\"h_name\":\"&\",\"h_data\":\"&\",\"c_data\":\"&\",\"text\":\"&\",\"facet_row\":\"&\",\"facet_col\":\"&\",\"facet_col_wrap\":\"&\",\"facet_row_spacing\":\"&\",\"facet_col_spacing\":\"&\",\"error_x\":\"&\",\"error_x_minus\":\"&\",\"error_y\":\"&\",\"error_y_minus\":\"&\",\"animation_frame\":\"&\",\"animation_group\":\"&\",\"range_color\":\"&\",\"opacity\":\"&\",\"size_max\":\"&\",\"marginal_x\":\"&\",\"marginal_y\":\"&\",\"log_x\":\"&\",\"log_y\":\"&\",\"range_x\":\"&\",\"range_y\":\"&\",\"render_mode\":\"&\",\"title\":\"&\",\"xtitle\":\"&\",\"ytitle\":\"&\",\"width\":\"&\",\"height\":\"&\"}\n",
    "                                                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781f48a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1aa928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498518db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953df368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b252f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7c1b75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785e5227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db301cde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d068a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a828ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadNP()\n",
    "print(type(data))\n",
    "print(data)\n",
    "print(len(data))\n",
    "plot3Dpn(data)\n",
    "tsne = TSNE(n_components=2,perplexity=2) # , random_state=100\n",
    "X_tsne = tsne.fit_transform(data)\n",
    "tsne.kl_divergence_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8481e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=X_tsne[:, 0], y=X_tsne[:, 1], color=np.array(range(69)))\n",
    "fig.update_layout(\n",
    "    title=\"t-SNE visualization of Custom Classification dataset\",\n",
    "    xaxis_title=\"First t-SNE\",\n",
    "    yaxis_title=\"Second t-SNE\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24dbc51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949f7744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb5e8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b33b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d814fa79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82cec2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e54728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd29853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83a56bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea116621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = cfn_field(\"prompts\",\"prompt_name\",\"email\",\"prompt_value\")\n",
    "prompt = cfn_index(\"prompts\",16,\"prompt_value\")\n",
    "# prompt = prompt_story\n",
    "# input_dict = {\"role\":role_list[0],\"model\":\"gpt-3.5-turbo-16k\",\"temperature\":0.5,\"max_tokens\":10000 ,\"n\":1,\"seed\":0,\"content\":prompt}\n",
    "input_dict = llmInputConf(prompt)\n",
    "out_raw = apply_completions(input_dict)\n",
    "out_dict = outputDictParseCompletion(out_raw) #model_list[0]\n",
    "final_dict = input_dict | out_dict\n",
    "\n",
    "print(final_dict[\"i_content\"])\n",
    "print(num_tokens_from_string(final_dict[\"i_content\"]))\n",
    "\n",
    "print(final_dict[\"o_content\"])\n",
    "print(num_tokens_from_string(final_dict[\"o_content\"]))\n",
    "\n",
    "final_dict[\"i_content\"] = \"\"\n",
    "final_dict[\"o_content\"] = \"\"\n",
    "print(final_dict.keys())\n",
    "\n",
    "print(final_dict)\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=\"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\",\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", #role_list[0],\n",
    "            \"content\": \"Say this is a test\"\n",
    "        }\n",
    "    ],\n",
    "    model=model_list[0],\n",
    "    temperature=0.5,\n",
    "    max_tokens=max_token,\n",
    "    n=1,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "print(str(chat_completion))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def chat_with_chatgpt(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    response = openai.Completion.create(\n",
    "        engine=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=100,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "\n",
    "    message = response.choices[0].text.strip()\n",
    "    return message\n",
    "\n",
    "user_prompt = \"Write a summary of the benefits of exercise.\"\n",
    "chatbot_response = chat_with_chatgpt(user_prompt)\n",
    "print(chatbot_response)\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  organization='org-JC0VmXs611nLY2FmI4JVhO5k',\n",
    ")\n",
    "\n",
    "        # print(dict_entry)\n",
    "#     hash_list = []\n",
    "#     for i in range(n) :\n",
    "#         text = openSTRtxt(folder_path+\"/\",file_list[index].replace(\".txt\",\"\"))\n",
    "#     print(text)\n",
    "#     return text\n",
    "# for i in range(10) :\n",
    "#     loadArticleFolder(i)\n",
    "\n",
    "\n",
    "def loadArticleFolder(length=0) :\n",
    "    folder_path = \"C:/Users/User/OneDrive/Desktop/article/file_2/article_download_main\"\n",
    "    root_path = Path(folder_path)\n",
    "    file_list = os.listdir(root_path)\n",
    "    print(len(file_list))\n",
    "    # text = openSTRtxt(folder_path,file_list[index])\n",
    "# for i in range(10) :\n",
    "#     prompt = str(loadArticleFolder(i))+\"\\n\"+parseList(text)\n",
    "#     print(prompt)\n",
    "loadArticleFolder(0)\n",
    "\n",
    "\n",
    "prompt_list = cfn_field(\"prompts\",\"prompt_type\",\"content\",\"prompt_value\",10) #\n",
    "df = getStandardDfComp()\n",
    "count = 0\n",
    "for prompt in prompt_list :\n",
    "    input_dict = llmInputConf(prompt)\n",
    "    out_raw = apply_completions(input_dict)\n",
    "    out_dict = outputDictParseCompletion(out_raw) #model_list[0]\n",
    "    final_dict = input_dict | out_dict\n",
    "    df = addDictToDF(df,final_dict)\n",
    "    if count%2 == 0 :\n",
    "        saveDFcsv(df, save_path, filename_save+str(count),True)\n",
    "    count = count + 1\n",
    "# prompt = cfn_field(\"prompts\",\"prompt_type\",\"prompt_name\")\n",
    "#print(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d2a505",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23110d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_small_answer =\"\"\"Answer all the following questions with a short answer.\n",
    "Your answer should be in csv format with a comma as separator and quotes around each fields.\n",
    "Here is the list of questions :\n",
    "What occasion corresponds to the longest day of the year?\n",
    "What is the distance from earth to the sun?\n",
    "What sport was featured on the first curved U.S. coin in 2014?\n",
    "Which country is the largest in the world?\n",
    "M&M’S Fruit Chews would eventually become what popular candy?\n",
    "According to Guinness World Records, what's the best-selling book of all time?\n",
    "What U.S. state is home to Acadia National Park?\n",
    "What is the only food that can never go bad?\n",
    "What was the first animal to ever be cloned?\n",
    "What is the name of the pet dinosaur on the TV cartoon 'The Flintstones'?\n",
    "What identity document is required to travel to different countries around the world?\n",
    "Who is considered the 'Father of Relativity?'\n",
    "Edie Falco and James Gandolfini star in what series about the life of a New Jersey mob boss?\n",
    "Nearly all fossils are preserved in what type of rock?\n",
    "What guitarist notably performed on the Michael Jackson song 'Beat It'?\n",
    "What is August’s birthstone?\n",
    "What is Prince Harry’s official first name?\n",
    "What is the fifth sign of the zodiac?\n",
    "Which branch of the U.S. armed forces used the slogan 'It’s not just a job, it’s an adventure'?\n",
    "By U.S. law, exit signs must be one of what two colors?\n",
    "What is an eight-sided shape called?\n",
    "\"\"\"\n",
    "\n",
    "prompt_yes_no =\"\"\"Answer all the following questions either \"True\" or \"False\".\n",
    "Your answer should be in csv format with a comma as separator and quotes around each fields.\n",
    "Here is the list of questions :\n",
    "Sharks are mammals.\n",
    "Sea otters have a favorite rock they use to break open food.\n",
    "The blue whale is the biggest animal to have ever lived.\n",
    "The hummingbird egg is the world's smallest bird egg.\n",
    "Pigs roll in the mud because they don't like being clean.\n",
    "Bats are blind.\n",
    "New York City is composed of between 36 and 42 islands.\n",
    "South Africa has one capital.\n",
    "The Atlantic Ocean is the biggest ocean on Earth.\n",
    "Mount Everest is the tallest mountain in the world.\n",
    "You can find the 'Desert of Death' in California.\n",
    "The total length of the Great Wall of China adds up to 13,171 miles.\n",
    "\"\"\"\n",
    "\n",
    "prompt_open =\"\"\"Answer all the following questions with maximum of 10 sentences per answer.\n",
    "Your answer should be in csv format with a comma as separator and quotes around each fields.\n",
    "Here is the list of questions :\n",
    "Can you help me plan a week's worth of dinner for two adults?\n",
    "Generate a meal plan for two days and give me the shopping list?\n",
    "I have tomato, lettuce, and broccoli. What can I prepare with them for a vegan lunch?\n",
    "What is an easy way to make a pasta recipe that features white sauce and mushroom?\n",
    "What would be a good bottle of wine to serve with Chicken roast dinner?\n",
    "I have only three ingredients - Onion, tomato, and spinach. Can you show me 3 meals that I can cook with these ingredients?\n",
    "What is a good food suggestion for someone who has had a bad day?\n",
    "I am a vegan and I am looking for healthy dinner ideas.\n",
    "Can you give a dessert suggestion on a stressful day?\n",
    "Suggest a multi-course dinner party menu with winter ingredients?\n",
    "Write a persuasive message to a potential employer explaining my relocation for a chef role?\n",
    "\"\"\"\n",
    "\n",
    "prompt_email = \"\"\"Write an email to a supervisor requesting time off for a vacation, including the dates of the requested time off and a plan for ensuring that work will be covered during your absence.\"\"\"\n",
    "\n",
    "prompt_story = \"\"\"Write a story about a young girl who discovers she has the power to control fire, but struggles with the responsibility that comes with it She must navigate the challenges of her new abilities while trying to keep her secret hidden from the world\"\"\"\n",
    "\n",
    "\n",
    "# Prompts closed answer\n",
    "prompt_date = \"Guess the exact date this article was written. I want you to give me an date with a specific year, month and day. The answer should only be this date in this format 'yyyy-mm-dd'.\"\n",
    "prompt_politics = \"Tell me if this article includes political topics. Answer with 'True' or 'False'.\"\n",
    "prompt_pos = \"Tell me if this article has a positive outlook. Answer with 'True' or 'False'.\"\n",
    "prompt_neg = \"Tell me if this article has a negative outlook. Answer with 'True' or 'False'.\"\n",
    "prompt_facts = \"Tell me if this article looks to be factual. Answer with 'True' or 'False'.\"\n",
    "prompt_sources = \"Tell me if this article includes sources on what is written. Answer with 'True' or 'False'.\"\n",
    "prompt_date_num = \"How many dates or time periods are referenced in this article. Answer with only the number of dates or time periods you found.\"\n",
    "prompt_spectrum=\"Guess on wich end of the political spectrum is the person who wrote this article situated.Answer with 'Left' or 'Right'.\"\n",
    "prompt_occurence=\"Tel me the percentage of appearence of this article's topîc.\"\n",
    "prompt_words=\"Tell me the 5 most cited words in the articles, except words like 'the', 'he', 'she', 'it', 'or', 'and', etc\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# df = pd.read_csv('output/embedded_1k_reviews.csv')\n",
    "# matrix = df.ada_embedding.apply(eval).to_list()\n",
    "val = openDFcsv(save_path,filename_save+\"final\")\n",
    "val = df[\"o_data\"].to_numpy()\n",
    "for ent in val :\n",
    "    new_list = ent.str.strip('()').str.split(',')\n",
    "# np_mat = df[\"embedding_list\"]\n",
    "# print(np_mat)\n",
    "# df[\"embedding_list\"] = df[\"o_data\"] .apply(np.array)\n",
    "# df = df[[\"embedding_list\"]].to_numpy()\n",
    "# matrix = df[\"o_data\"].apply(np.array)\n",
    "\n",
    "#.str.strip('()').str.split(',')\n",
    "# print(val.dtypes)\n",
    "# display(val)\n",
    "\n",
    "print(type(val))\n",
    "print(val.shape)\n",
    "print(val)\n",
    "\n",
    "\n",
    "# # Create a t-SNE model and transform the data\n",
    "# tsne = TSNE(n_components=2, perplexity=15, random_state=42, init='random', learning_rate=200)\n",
    "# vis_dims = tsne.fit_transform(matrix)\n",
    "\n",
    "# colors = [\"red\", \"darkorange\", \"gold\", \"turquiose\", \"darkgreen\"]\n",
    "# x = [x for x,y in vis_dims]\n",
    "# y = [y for x,y in vis_dims]\n",
    "# color_indices = df.Score.values - 1\n",
    "\n",
    "# colormap = matplotlib.colors.ListedColormap(colors)\n",
    "# plt.scatter(x, y, c=color_indices, cmap=colormap, alpha=0.3)\n",
    "# plt.title(\"Amazon ratings visualized in language using t-SNE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66791f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "\n",
    "df = openDFcsv(save_path,filename_save+\"final\")\n",
    "new = df.embedding.apply(literal_eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd3ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.embeddings_utils import get_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed1cb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast.literal_eval\n",
    "val = openDFcsv(save_path,filename_save+\"final\")\n",
    "df.o_data = df.o_data.apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26867783",
   "metadata": {},
   "outputs": [],
   "source": [
    "li = [[0.1,0.2,0.3],[0.1,0.2,0.3]]\n",
    "li2 = np.array(li)\n",
    "print(li2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7877850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "li1 = np.ndarray((10,10))\n",
    "print(li1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc56b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aa21a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f9305a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1eb8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815954ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c49c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = getStandardDfComp()\n",
    "df = getStandardDfEmb()\n",
    "getNumberOfArticles(open_path)\n",
    "filename_list = loadArticleFolderList(open_path,5000) # [\"fa897c02295f34ce2e15f602769edf204ea00be7.txt\"]\n",
    "article_dict_list = loadListArticleHash(open_path,filename_list)\n",
    "count = 0\n",
    "for article_dict in article_dict_list :\n",
    "    # stats_dict = getStatsOnArticleText(article_dict[\"text\"])\n",
    "    article_dict[\"text\"] = textListToText(article_dict[\"text\"])\n",
    "    # print(article_dict[\"hash\"])\n",
    "    # print(stats_dict)\n",
    "        # llm_input_dict = llmInputConfArticle(str(article_dict[\"text\"]),prompt_spectrum)\n",
    "        # out_raw = apply_completions(llm_input_dict)\n",
    "        # out_dict = outputDictParseCompletion(out_raw) #model_list[0]\n",
    "    input_dict = llmInputConfEmbeddings(article_dict[\"text\"],dimensions=100)\n",
    "    out_raw = apply_embeddings(input_dict)\n",
    "    out_dict = outputDictParseEmbeddings(out_raw) #model_list[0]\n",
    "    # mat.append(out_dict[\"o_data\"])\n",
    "    del article_dict[\"text\"]\n",
    "    del input_dict[\"i_text\"]\n",
    "    final_dict = article_dict | out_dict | input_dict\n",
    "    # final_dict[\"content\"] = \"\"\n",
    "    # final_dict[\"hash_key\"] = article_dict[\"hash\"]\n",
    "    df = addDictToDF(df,final_dict,select_fields_emb)\n",
    "    \n",
    "    if count%5 == 0 :\n",
    "        saveDFcsv(df, save_path, filename_save+str(count),True)\n",
    "    count = count + 1\n",
    "    # print(final_dict.keys())\n",
    "    # print(final_dict)\n",
    "saveDFcsv(df, save_path, filename_save+\"final\",True)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc25bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7569fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1620b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def testQuestionsBatchEmbedding():\n",
    "#     max_prompt=10 # 100\n",
    "#     dim=10 # 100\n",
    "#     save_every = 1000\n",
    "#     df = None\n",
    "#     # mat = []\n",
    "#     prompt_list = getDataToQuerryListLLM(max_prompt,True)\n",
    "#     # prompt_list = cfn_field(\"prompts\",\"prompt_type\",\"content\",\"prompt_value\",max_prompt) #\n",
    "#     # dfemb = getStandardDfnumColumn(dim)\n",
    "#     count = 0\n",
    "#     for prompt in prompt_list :\n",
    "#         print(prompt[\"hash_key\"])\n",
    "#         input_dict = llmInputConfEmbeddings(prompt[\"text\"],dimensions=dim,hash_key=prompt[\"hash_key\"])\n",
    "#         out_raw = apply_embeddings(input_dict)\n",
    "#         out_dict = outputDictParseEmbeddings(out_raw)\n",
    "#         # mat.append(out_dict[\"o_data\"])\n",
    "#         final_dict = input_dict | out_dict\n",
    "#         df = addDictToDF(df,final_dict,selected_fields_emp)\n",
    "#         if count%save_every == 0 and count != 0:\n",
    "#             saveDFcsv(df.set_index('hash_key'), save_path, filename_save+str(count),True)\n",
    "#         count = count + 1\n",
    "#     saveDFcsv(df.set_index('hash_key'), save_path, filename_save+\"final_test_emb\",True)\n",
    "#     return df\n",
    "\n",
    "# df = testQuestionsBatchEmbedding()\n",
    "# display(df)\n",
    "\n",
    "\n",
    "def testEmbeddings():\n",
    "    max_prompt =100\n",
    "    dim=100\n",
    "    save_every = 0\n",
    "    mat = []\n",
    "    prompt_list = cfn_field(\"prompts\",\"prompt_type\",\"content\",\"prompt_value\",max_prompt) #\n",
    "    df = getStandardDfEmb()\n",
    "    # dfemb = getStandardDfnumColumn(dim)\n",
    "    count = 0\n",
    "    for prompt in prompt_list :\n",
    "        input_dict = llmInputConfEmbeddings(prompt,dimensions=dim)\n",
    "        out_raw = apply_embeddings(input_dict)\n",
    "        out_dict = outputDictParseEmbeddings(out_raw) #model_list[0]\n",
    "        mat.append(out_dict[\"o_data\"])\n",
    "        final_dict = input_dict | out_dict\n",
    "        df = addDictToDF(df,final_dict)\n",
    "        # if count%save_every == 0 :\n",
    "            # saveDFcsv(df, save_path, filename_save+str(count),True)\n",
    "        count = count + 1\n",
    "    # saveDFcsv(df, save_path, filename_save+\"final\",True)\n",
    "    # display(df)\n",
    "    # df_em = pd.DataFrame(np.array(mat))\n",
    "    # ndarr = np.ndarray(mat)\n",
    "    # display(df_em)\n",
    "    # df_out = df.join(df_em, how=\"inner\")\n",
    "    # df_out[\"o_data\"] = \"\"\n",
    "    return mat\n",
    "main_out = testEmbeddings()\n",
    "np_out = np.array(main_out)\n",
    "saveNP(np_out)\n",
    "\n",
    "def testQuestionsDifferentParameters():\n",
    "    model_list = [0,1,2]\n",
    "    temperature_list = [0,0.25,0.5,0.75,1]\n",
    "    max_prompt = 100\n",
    "    save_every = 5\n",
    "    prompt_list = cfn_field(\"prompts\",\"prompt_type\",\"content\",\"prompt_value\",max_prompt) #\n",
    "    df = getStandardDfComp()\n",
    "    count = 0\n",
    "    for prompt in prompt_list :\n",
    "        for model_n in model_list:\n",
    "            for temperature_n in temperature_list :\n",
    "                input_dict = llmInputConf(prompt,model_num=model_n,temperature=temperature_n)\n",
    "                out_raw = apply_completions(input_dict)\n",
    "                out_dict = outputDictParseCompletion(out_raw) #model_list[0]\n",
    "                final_dict = input_dict | out_dict\n",
    "                df = addDictToDF(df,final_dict)\n",
    "                if count%save_every == 0 :\n",
    "                    saveDFcsv(df, save_path, filename_save+str(count),True)\n",
    "                count = count + 1\n",
    "    saveDFcsv(df, save_path, filename_save+\"final\",True)\n",
    "# testQuestionsDifferentParameters()\n",
    "\n",
    "# df = openDFcsv(path_model_list,filename_model_lis)\n",
    "# display(df)\n",
    "\n",
    "getNumberOfArticles(open_path)\n",
    "filename_list = loadArticleFolderList(open_path,20)\n",
    "article_dict_list = loadListArticleHash(open_path,filename_list)\n",
    "for article_dict in article_dict_list :\n",
    "    stats_dict = getStatsOnArticleText(article_dict[\"text\"])\n",
    "    print(article_dict[\"hash_key\"])\n",
    "    print(stats_dict)\n",
    "    \n",
    "    \n",
    "# df = getStandardDfComp()\n",
    "df = getStandardDfEmb()\n",
    "getNumberOfArticles(open_path)\n",
    "filename_list = loadArticleFolderList(open_path,5000) # [\"fa897c02295f34ce2e15f602769edf204ea00be7.txt\"]\n",
    "article_dict_list = loadListArticleHash(open_path,filename_list)\n",
    "count = 0\n",
    "for article_dict in article_dict_list :\n",
    "    # stats_dict = getStatsOnArticleText(article_dict[\"text\"])\n",
    "    article_dict[\"text\"] = textListToText(article_dict[\"text\"])\n",
    "    # print(article_dict[\"hash\"])\n",
    "    # print(stats_dict)\n",
    "        # llm_input_dict = llmInputConfArticle(str(article_dict[\"text\"]),prompt_spectrum)\n",
    "        # out_raw = apply_completions(llm_input_dict)\n",
    "        # out_dict = outputDictParseCompletion(out_raw) #model_list[0]\n",
    "    input_dict = llmInputConfEmbeddings(article_dict[\"text\"],dimensions=100)\n",
    "    out_raw = apply_embeddings(input_dict)\n",
    "    out_dict = outputDictParseEmbeddings(out_raw) #model_list[0]\n",
    "    # mat.append(out_dict[\"o_data\"])\n",
    "    del article_dict[\"text\"]\n",
    "    del input_dict[\"i_text\"]\n",
    "    final_dict = article_dict | out_dict | input_dict\n",
    "    # final_dict[\"content\"] = \"\"\n",
    "    # final_dict[\"hash_key\"] = article_dict[\"hash\"]\n",
    "    df = addDictToDF(df,final_dict,select_fields_emb)\n",
    "    \n",
    "    if count%5 == 0 :\n",
    "        saveDFcsv(df, save_path, filename_save+str(count),True)\n",
    "    count = count + 1\n",
    "    # print(final_dict.keys())\n",
    "    # print(final_dict)\n",
    "saveDFcsv(df, save_path, filename_save+\"final\",True)\n",
    "display(df)\n",
    "\n",
    "# \n",
    "# out_dict = outputDictParseCompletion(out_raw) #model_list[0]\n",
    "# final_dict = input_dict | out_dict\n",
    "\n",
    "\n",
    "# out_raw = outputDictParseCompletion(apply_completions(input_dict))\n",
    "# out_dict = outputDictParseCompletion(out_raw) #model_list[0]\n",
    "# final_dict = input_dict | out_dict \n",
    "input_comp=llmInputConf(\"By U.S. law, exit signs must be one of what two colors?\")\n",
    "out_dict_comp = outputDictParseCompletion(apply_completions(input_comp))\n",
    "\n",
    "input_embed = llmInputConfEmbeddings(\"Your text string goes here\")\n",
    "out_dict_embed = outputDictParseEmbeddings(apply_embeddings(input_embed))\n",
    "print(out_dict_comp | input_comp)\n",
    "print(out_dict_embed | input_embed)\n",
    "print(type(out_dict_embed[\"o_data\"]))\n",
    "# response = llm_client.embeddings.create(\n",
    "#     input=\"Your text string goes here\",\n",
    "#     model=\"text-embedding-3-small\"\n",
    "# )\n",
    "# print(response)\n",
    "\n",
    "df = openDFcsv(save_path,filename_save+\"final\")\n",
    "display(df)\n",
    "print(df.dtypes)\n",
    "\n",
    "C:\\Users\\User\\OneDrive\\Desktop\\article\\file_2\\.bin\\amazon\n",
    "    \n",
    "import plotly.express as px\n",
    "def testEmbeddings():\n",
    "    max_prompt =100\n",
    "    dim=100\n",
    "    save_every = 1000\n",
    "    mat = []\n",
    "    prompt_list = cfn_field(\"prompts\",\"prompt_type\",\"content\",\"prompt_value\",max_prompt) #\n",
    "    df = getStandardDfEmb()\n",
    "    # dfemb = getStandardDfnumColumn(dim)\n",
    "    count = 0\n",
    "    for prompt in prompt_list :\n",
    "        input_dict = llmInputConfEmbeddings(prompt,dimensions=dim)\n",
    "        out_raw = apply_embeddings(input_dict)\n",
    "        out_dict = outputDictParseEmbeddings(out_raw) #model_list[0]\n",
    "        mat.append(out_dict[\"o_data\"])\n",
    "        final_dict = input_dict | out_dict\n",
    "        df = addDictToDF(df,final_dict)\n",
    "        # if count%save_every == 0 :\n",
    "            # saveDFcsv(df, save_path, filename_save+str(count),True)\n",
    "        count = count + 1\n",
    "    # saveDFcsv(df, save_path, filename_save+\"final\",True)\n",
    "    # display(df)\n",
    "    # df_em = pd.DataFrame(np.array(mat))\n",
    "    # ndarr = np.ndarray(mat)\n",
    "    # display(df_em)\n",
    "    # df_out = df.join(df_em, how=\"inner\")\n",
    "    # df_out[\"o_data\"] = \"\"\n",
    "    return mat\n",
    "main_out = testEmbeddings()\n",
    "np_out = np.array(main_out)\n",
    "saveNP(np_out)\n",
    "\n",
    "print(main_out)\n",
    "print(type(main_out))\n",
    "\n",
    "print(np_out)\n",
    "print(type(np_out))\n",
    "print(np_out.shape)\n",
    "\n",
    "print(b)\n",
    "\n",
    "\n",
    "print(type(X_tsne))\n",
    "print(X_tsne.shape)\n",
    "\n",
    "\n",
    "\n",
    "X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "model.fit_transform(X) \n",
    "\n",
    "\n",
    "loadArticleFolder()\n",
    "open_path = \"C:/Users/User/OneDrive/Desktop/article/file_2/article_download_main/\"\n",
    "filename=\"4d54d5722e8e03a1e76159c2594ab4c83327a749\"\n",
    "text = openSTRtxt(open_path,filename)\n",
    "prompt = str(prompt_date)+\"\\n\"+parseList(text)\n",
    "print(prompt)\n",
    "\n",
    "print(final_dict[\"content_o\"])\n",
    "print(\"\\n\\n\\n\",final_dict[\"token_p\"])\n",
    "print(final_dict[\"token_c\"])\n",
    "print(final_dict[\"token_t\"])\n",
    "print(final_dict)\n",
    "\n",
    "open_path = \"C:/Users/User/OneDrive/Desktop/article/file_2/article_download_main\"\n",
    "\n",
    "\n",
    "\n",
    "filename=\"4d54d5722e8e03a1e76159c2594ab4c83327a749\"\n",
    "text = openSTRtxt(open_path,filename)\n",
    "prompt = str(prompt_date)+\"\\n\"+parseList(text)\n",
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
